import os
from neo4j import GraphDatabase
import shutil
from dotenv import load_dotenv
import gradio as gr
import gc
import time
import uuid
import torch
import requests
import json
import traceback
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from rag_manager import RAGManager
from langchain_community.chat_models.ollama import ChatOllama
from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler
import asyncio
import soundfile as sf 
# Voice processing imports
import whisper
import tempfile
import numpy as np
from gtts import gTTS
import pygame
from io import BytesIO
import threading

rag_manager = None 

# Load environment variables
load_dotenv()

# Voice processing setup
whisper_model = None
pygame_initialized = False

def initialize_voice_components():
    """Initialize Whisper and pygame for voice processing"""
    global whisper_model, pygame_initialized
    
    try:
        print("ğŸ¤ Loading Whisper model...")
        whisper_model = whisper.load_model("base")
        print("âœ… Whisper model loaded successfully")
    except Exception as e:
        print(f"âŒ Error loading Whisper: {e}")
        whisper_model = None
    
    try:
        pygame.mixer.init()
        pygame_initialized = True
        print("âœ… Pygame audio initialized")
    except Exception as e:
        print(f"âŒ Error initializing pygame: {e}")
        pygame_initialized = False

def transcribe_audio(audio_file):
    """Transcribe audio file using Whisper"""
    global whisper_model

    if not whisper_model:
        return "âŒ Whisper model not loaded"

    if not os.path.exists(audio_file):
        return f"âŒ Audio file not found: {audio_file}"

    try:
        print(f"ğŸ¤ Transcribing audio file: {audio_file}")
        result = whisper_model.transcribe(audio_file)
        transcribed_text = result["text"].strip()
        print(f"ğŸ“ Transcribed: {transcribed_text}")
        return transcribed_text
    except Exception as e:
        error_msg = f"âŒ Error transcribing audio: {str(e)}"
        print(error_msg)
        return error_msg

def process_voice_input(audio):
    """Process voice input and return transcribed text"""
    if audio is None:
        return "âŒ No audio file provided"
    
    try:
        # Check if audio is a tuple (numpy array, sample rate) or file path
        if isinstance(audio, tuple) and len(audio) == 2:
            # It's a numpy array with sample rate
            data, samplerate = audio
            
            # Check if data is valid
            if data is None or len(data) == 0:
                return "âŒ Empty audio data"
            
            # Create temporary file
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
                sf.write(tmp.name, data, samplerate)
                transcribed_text = transcribe_audio(tmp.name)
                os.unlink(tmp.name)
                return transcribed_text
        
        elif isinstance(audio, str) and os.path.exists(audio):
            # It's a file path
            return transcribe_audio(audio)
        
        else:
            return "âŒ Invalid audio format"
            
    except Exception as e:
        error_msg = f"âŒ Error processing voice input: {str(e)}"
        print(error_msg)
        print(traceback.format_exc())
        return error_msg

def text_to_speech(text, lang='en'):
    """Convert text to speech using gTTS and play it"""
    global pygame_initialized
    
    if not pygame_initialized:
        return "âŒ Audio system not initialized"
    
    try:
        print(f"ğŸ”Š Converting text to speech: {text[:50]}...")
        
        # Create TTS object
        tts = gTTS(text=text, lang=lang, slow=False)
        
        # Save to temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmp_file:
            tts.save(tmp_file.name)
            tmp_filename = tmp_file.name
        
        # Play the audio
        pygame.mixer.music.load(tmp_filename)
        pygame.mixer.music.play()
        
        # Wait for playback to finish
        while pygame.mixer.music.get_busy():
            time.sleep(0.1)
        
        # Clean up temporary file
        os.unlink(tmp_filename)
        
        print("âœ… Text-to-speech playback completed")
        return "âœ… Audio played successfully"
        
    except Exception as e:
        error_msg = f"âŒ Error in text-to-speech: {str(e)}"
        print(error_msg)
        return error_msg

# Neo4j connection setup
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "password")

neo4j_driver = None
try:
    neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
    # Test the connection
    with neo4j_driver.session() as session:
        session.run("RETURN 1")
    print("âœ… Connected to Neo4j successfully")
except Exception as e:
    print(f"âš ï¸ Could not connect to Neo4j: {e}")
    neo4j_driver = None

# Ollama configuration
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "gemma:2b"

BASE_PERSIST_DIR = "./graph_db"
BASE_VECTOR_DIR = "./vector_db"

# Initialize device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ Using device: {device}")

# Import your custom modules with better error handling
MODULES_AVAILABLE = True
missing_modules = []

try:
    from url_text_extractor import extract_text_from_url
except ImportError as e:
    missing_modules.append("url_text_extractor")
    print(f"âš ï¸ Warning: url_text_extractor not found: {e}")

try:
    from document_loader import load_documents_from_files
except ImportError as e:
    missing_modules.append("document_loader")
    print(f"âš ï¸ Warning: document_loader not found: {e}")

try:
    from graph_rag_manager import GraphRAGManager
except ImportError as e:
    missing_modules.append("GraphRAGManager")
    print(f"âš ï¸ Warning: GraphRAGManager not found: {e}")
    MODULES_AVAILABLE = False

try:
    from rag_chain import generate_graph_answer_ollama
except ImportError as e:
    missing_modules.append("rag_chain")
    print(f"âš ï¸ Warning: rag_chain not found: {e}")

try:
    from rag import generate_answer_ollama
except ImportError as e:
    missing_modules.append("rag")
    print(f"âš ï¸ Warning: rag not found: {e}")

if missing_modules:
    print(f"âš ï¸ Missing modules: {', '.join(missing_modules)}")

# Global variables
graph_rag_manager = None
embeddings = None
current_persist_dir = None
current_vector_dir = None
current_rag_type = None
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

def check_ollama_connection():
    """Check if Ollama is running and model is available"""
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [model['name'] for model in models]
            if OLLAMA_MODEL in model_names or any(OLLAMA_MODEL in name for name in model_names):
                return True, f"âœ… Ollama connected. Available models: {', '.join(model_names[:3])}"
            else:
                return False, f"âŒ Model '{OLLAMA_MODEL}' not found. Available: {', '.join(model_names[:3])}\nRun: ollama pull {OLLAMA_MODEL}"
        return False, "âŒ Ollama not responding"
    except requests.exceptions.ConnectionError:
        return False, f"âŒ Cannot connect to Ollama at {OLLAMA_BASE_URL}\nMake sure Ollama is running: ollama serve"
    except requests.exceptions.Timeout:
        return False, "âŒ Ollama connection timeout"
    except Exception as e:
        return False, f"âŒ Ollama connection failed: {str(e)}"

def get_graph_rag_context(question):
    """Get relevant context from GraphRAG system"""
    global graph_rag_manager
    
    try:
        if not graph_rag_manager:
            return "No GraphRAG context available"
        
        # Method 1: Try to get relevant context from the graph
        context = ""
        
        # Try to get knowledge graph store
        if hasattr(graph_rag_manager, 'knowledge_graph_store'):
            kg_store = graph_rag_manager.knowledge_graph_store
            
            # Try to get triplets
            if hasattr(kg_store, 'get_triplets'):
                triplets = kg_store.get_triplets()
                # Filter relevant triplets based on question keywords
                question_words = question.lower().split()
                relevant_triplets = []
                
                for triplet in triplets[:100]:  # Limit to avoid too much context
                    triplet_text = ' '.join([str(x) for x in triplet]).lower()
                    if any(word in triplet_text for word in question_words):
                        relevant_triplets.append(triplet)
                
                if relevant_triplets:
                    context = "Relevant knowledge from graph:\n"
                    for triplet in relevant_triplets[:15]:  # Top 15 most relevant
                        context += f"- {triplet[0]} -> {triplet[1]} -> {triplet[2]}\n"
        
        # Method 2: Try to get documents or chunks
        if not context and hasattr(graph_rag_manager, 'vector_store'):
            vector_store = graph_rag_manager.vector_store
            if hasattr(vector_store, 'similarity_search'):
                docs = vector_store.similarity_search(question, k=3)
                context = "Relevant documents:\n"
                for doc in docs:
                    context += f"- {doc.page_content[:500]}...\n"
        
        return context if context else "No relevant GraphRAG context found"
        
    except Exception as e:
        print(f"Error getting GraphRAG context: {e}")
        return f"Error retrieving GraphRAG context: {str(e)}"

def get_rag_context(question, k=3):
    """Get relevant context from the RAG vector store."""
    global rag_manager
    try:
        if rag_manager and rag_manager.is_ready():
            docs = rag_manager.vector_store.similarity_search(question, k=k)
            context = "\n\n".join([doc.page_content for doc in docs])
            return context
        else:
            return "No RAG context available"
    except Exception as e:
        print(f"Error getting RAG context: {e}")
        return "Error retrieving context"

def stream_ollama_response(prompt):
    """Stream response from Ollama using direct API calls"""
    try:
        url = f"{OLLAMA_BASE_URL}/api/generate"
        payload = {
            "model": OLLAMA_MODEL,
            "prompt": prompt,
            "stream": True
        }
        
        response = requests.post(url, json=payload, stream=True, timeout=30)
        
        if response.status_code != 200:
            yield f"âŒ Error: Ollama responded with status {response.status_code}"
            return
        
        full_response = ""
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line)
                    if 'response' in data:
                        chunk = data['response']
                        full_response += chunk
                        yield full_response
                    if data.get('done', False):
                        break
                except json.JSONDecodeError:
                    continue
                    
    except requests.exceptions.RequestException as e:
        yield f"âŒ Connection error: {str(e)}"
    except Exception as e:
        yield f"âŒ Unexpected error: {str(e)}"

def stream_rag_answer(question):
    """Stream RAG answer with improved error handling"""
    global current_rag_type
    
    try:
        yield "ğŸ” Retrieving relevant context...\n\n"
        
        # Get context based on RAG type
        if current_rag_type == "GraphRAG":
            context = get_graph_rag_context(question)
            yield f"ğŸ” Found GraphRAG context, generating answer...\n\n"
        else:
            context = get_rag_context(question)
            yield f"ğŸ” Found RAG context, generating answer...\n\n"
        
        # Create prompt
        prompt = f"""Based on the following context, please answer the question comprehensively and accurately.

Context:
{context}

Question: {question}

Answer:"""

        # Stream the response
        answer_started = False
        for response in stream_ollama_response(prompt):
            if not answer_started:
                yield f"ğŸ” Context retrieved, generating answer...\n\n**Answer:**\n{response}"
                answer_started = True
            else:
                yield f"ğŸ” Context retrieved, generating answer...\n\n**Answer:**\n{response}"
                
    except Exception as e:
        error_msg = f"âŒ Error in stream_rag_answer: {str(e)}"
        print(error_msg)
        print(traceback.format_exc())
        yield error_msg

def on_ask_with_voice(question, enable_tts=False):
    """Handle question answering with optional text-to-speech output"""
    global graph_rag_manager, current_persist_dir, current_vector_dir, current_rag_type, rag_manager
    
    if not question or not question.strip():
        error_msg = "âŒ Please enter a question"
        if enable_tts:
            threading.Thread(target=text_to_speech, args=(error_msg,)).start()
        yield error_msg
        return
    
    if not MODULES_AVAILABLE:
        error_msg = "âŒ Required modules not found. Please ensure all custom modules are available."
        if enable_tts:
            threading.Thread(target=text_to_speech, args=(error_msg,)).start()
        yield error_msg
        return
    
    if not current_rag_type:
        error_msg = "âš ï¸ Please upload documents first to build the knowledge base."
        if enable_tts:
            threading.Thread(target=text_to_speech, args=(error_msg,)).start()
        yield error_msg
        return
    
    ollama_status, ollama_msg = check_ollama_connection()
    if not ollama_status:
        if enable_tts:
            threading.Thread(target=text_to_speech, args=(f"Ollama connection failed: {ollama_msg}",)).start()
        yield f"ğŸ”Œ {ollama_msg}"
        return
    
    try:
        if current_rag_type == "GraphRAG":
            # Use GraphRAG
            if not graph_rag_manager:
                if current_persist_dir and os.path.exists(current_persist_dir):
                    try:
                        loading_msg = "Loading GraphRAG knowledge base..."
                        yield f"ğŸ”„ {loading_msg}\n\n"
                        if enable_tts:
                            threading.Thread(target=text_to_speech, args=(loading_msg,)).start()
                        
                        print(f"ğŸ”„ Loading GraphRAG from {current_persist_dir}")
                        graph_rag_manager = GraphRAGManager(
                            persist_dir=current_persist_dir,
                            api_key=None,
                            device=device
                        )
                        
                        # Try to load the knowledge graph
                        if hasattr(graph_rag_manager, 'load_knowledge_graph'):
                            graph_rag_manager.load_knowledge_graph()
                            print("âœ… Knowledge graph loaded successfully")
                        else:
                            print("âš ï¸ GraphRAGManager doesn't have load_knowledge_graph method")
                            
                    except Exception as e:
                        error_msg = f"âš ï¸ Error loading knowledge graph: {str(e)}"
                        print(error_msg)
                        traceback.print_exc()
                        if enable_tts:
                            threading.Thread(target=text_to_speech, args=(error_msg,)).start()
                        yield error_msg
                        return
                else:
                    error_msg = "âš ï¸ Please upload content first to build the GraphRAG knowledge graph."
                    if enable_tts:
                        threading.Thread(target=text_to_speech, args=(error_msg,)).start()
                    yield error_msg
                    return
            
            # Stream GraphRAG answer with TTS
            try:
                full_answer = ""
                for response in stream_rag_answer(question):
                    yield response
                    # Extract just the answer part for TTS
                    if "**Answer:**\n" in response:
                        full_answer = response.split("**Answer:**\n", 1)[1].strip()
                
                # Convert final answer to speech
                if enable_tts and full_answer:
                    threading.Thread(target=text_to_speech, args=(full_answer,)).start()
                    
            except Exception as e:
                error_msg = f"âŒ Error generating GraphRAG answer: {str(e)}"
                print(error_msg)
                traceback.print_exc()
                if enable_tts:
                    threading.Thread(target=text_to_speech, args=(error_msg,)).start()
                yield error_msg
        
        else:  # Regular RAG
            # Check if vector store is loaded
            try:
                if not rag_manager or not rag_manager.is_ready():
                    if current_vector_dir and os.path.exists(current_vector_dir):
                        loading_msg = "Loading RAG vector store..."
                        yield f"ğŸ”„ {loading_msg}\n\n"
                        if enable_tts:
                            threading.Thread(target=text_to_speech, args=(loading_msg,)).start()
                        
                        rag_manager = RAGManager(vector_store_path=current_vector_dir)
                        if not rag_manager.load_vector_store():
                            error_msg = "âŒ Failed to load vector store"
                            if enable_tts:
                                threading.Thread(target=text_to_speech, args=(error_msg,)).start()
                            yield error_msg
                            return
                    else:
                        error_msg = "âš ï¸ Please upload content first to build the RAG vector store."
                        if enable_tts:
                            threading.Thread(target=text_to_speech, args=(error_msg,)).start()
                        yield error_msg
                        return
                        
                # Stream RAG answer with TTS
                full_answer = ""
                for response in stream_rag_answer(question):
                    yield response
                    # Extract just the answer part for TTS
                    if "**Answer:**\n" in response:
                        full_answer = response.split("**Answer:**\n", 1)[1].strip()
                
                # Convert final answer to speech
                if enable_tts and full_answer:
                    threading.Thread(target=text_to_speech, args=(full_answer,)).start()
                    
            except Exception as e:
                error_msg = f"âŒ Error generating RAG answer: {str(e)}"
                print(error_msg)
                traceback.print_exc()
                if enable_tts:
                    threading.Thread(target=text_to_speech, args=(error_msg,)).start()
                yield error_msg
    
    except Exception as e:
        error_msg = f"âŒ Unexpected error: {str(e)}"
        print(error_msg)
        traceback.print_exc()
        if enable_tts:
            threading.Thread(target=text_to_speech, args=(error_msg,)).start()
        yield error_msg

def handle_inputs(urls, files, rag_type, progress=gr.Progress()):
    """
    Handles building the knowledge base from URLs and files for RAG or GraphRAG.
    """
    global rag_manager, graph_rag_manager, current_persist_dir, current_vector_dir, current_rag_type

    try:
        progress(0, desc="Starting knowledge base build...")

        # Parse URLs
        url_list = []
        if urls:
            url_list = [u.strip() for u in urls.split(",") if u.strip()]
        docs = []

        # Extract text from URLs
        if url_list:
            progress(0.1, desc="Extracting text from URLs...")
            for url in url_list:
                try:
                    text = extract_text_from_url(url)
                    docs.append({"content": text, "source": url})
                except Exception as e:
                    yield f"âŒ Error extracting from {url}: {e}"

        # Load files
        file_paths = []
        if files:
            if isinstance(files, list):
                file_paths = [f.name if hasattr(f, "name") else f for f in files]
            else:
                file_paths = [files.name if hasattr(files, "name") else files]
        if file_paths:
            progress(0.2, desc="Loading files...")
            loaded_docs = load_documents_from_files(file_paths)
            for doc in loaded_docs:
                docs.append({"content": doc.page_content, "source": doc.metadata.get("source", "uploaded file")})

        if not docs:
            yield "âš ï¸ No documents found. Please provide URLs or upload files."
            return

        # Build knowledge base
        if rag_type == "GraphRAG":
            progress(0.4, desc="Building GraphRAG knowledge base...")
            persist_dir = os.path.join(BASE_PERSIST_DIR, f"graphrag_{uuid.uuid4().hex[:8]}")
            os.makedirs(persist_dir, exist_ok=True)
            graph_rag_manager = GraphRAGManager(persist_dir=persist_dir, device=device)
            status = graph_rag_manager.build_knowledge_graph([Document(page_content=d["content"], metadata={"source": d["source"]}) for d in docs])
            current_persist_dir = persist_dir
            current_rag_type = "GraphRAG"
            yield f"âœ… GraphRAG knowledge base built!\n{status}"
        else:
            progress(0.4, desc="Building RAG vector store...")
            vector_dir = os.path.join(BASE_VECTOR_DIR, f"rag_{uuid.uuid4().hex[:8]}")
            os.makedirs(vector_dir, exist_ok=True)
            rag_manager = RAGManager(vector_store_path=vector_dir)
            success = rag_manager.create_vector_store(docs)
            current_vector_dir = vector_dir
            current_rag_type = "RAG"
            if success:
                yield f"âœ… RAG vector store built with {len(docs)} documents."
            else:
                yield "âŒ Failed to build RAG vector store."

    except Exception as e:
        error_msg = f"âŒ Error building knowledge base: {str(e)}\n{traceback.format_exc()}"
        print(error_msg)
        yield error_msg

def get_current_status():
    """Get current system status for display in Gradio."""
    global current_rag_type, current_persist_dir, current_vector_dir, graph_rag_manager, rag_manager, neo4j_driver

    status_lines = []

    # Ollama status
    ollama_status, ollama_msg = check_ollama_connection()
    status_lines.append(f"ğŸ¤– Ollama: {'âœ… Connected' if ollama_status else 'âŒ Not Connected'}")
    if not ollama_status:
        status_lines.append(f"   {ollama_msg}")

    # Neo4j status
    neo4j_status = "âœ… Connected" if neo4j_driver else "âŒ Not Connected"
    status_lines.append(f"ğŸ—„ï¸ Neo4j: {neo4j_status}")

    # Voice system status
    status_lines.append(f"ğŸ¤ Whisper: {'âœ… Loaded' if whisper_model else 'âŒ Not Loaded'}")
    status_lines.append(f"ğŸ”Š Audio System: {'âœ… Ready' if pygame_initialized else 'âŒ Not Ready'}")

    # RAG/GraphRAG status
    if current_rag_type:
        status_lines.append(f"ğŸ§  RAG Type: {current_rag_type}")
        if current_rag_type == "GraphRAG":
            status_lines.append(f"ğŸ“ GraphRAG Dir: {current_persist_dir}")
            if graph_rag_manager:
                status_lines.append("ğŸ“Š GraphRAG: âœ… Loaded")
                if hasattr(graph_rag_manager, "get_graph_stats"):
                    stats = graph_rag_manager.get_graph_stats()
                    status_lines.append(f"   Nodes: {stats.get('nodes', 0)}, Edges: {stats.get('edges', 0)}, Chunks: {stats.get('chunks', 0)}")
            else:
                status_lines.append("ğŸ“Š GraphRAG: âš ï¸ Not Loaded")
        else:
            status_lines.append(f"ğŸ“ Vector Dir: {current_vector_dir}")
            if rag_manager and hasattr(rag_manager, "get_stats"):
                stats = rag_manager.get_stats()
                status_lines.append(f"   Chunks: {stats.get('num_chunks', 0)}, Vector Dim: {stats.get('vector_dimension', 0)}")
            else:
                status_lines.append("ğŸ“Š RAG: âš ï¸ Not Loaded")
    else:
        status_lines.append("ğŸ§  RAG Type: Not selected or built yet.")

    return "\n".join(status_lines)

def create_interface():
    """Create and configure the Gradio interface with voice features"""
    
    with gr.Blocks(title="RAG & Graph RAG System with Voice", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ¤– RAG & Graph RAG System with Voice Features")
        gr.Markdown("Upload documents, ask questions via text or voice, and get audio responses!")
        
        with gr.Tab("ğŸ“¤ Upload & Build"):
            with gr.Row():
                with gr.Column(scale=1):
                    urls_input = gr.Textbox(
                        label="ğŸŒ URLs (comma-separated)",
                        placeholder="https://example.com, https://another-site.com",
                        lines=2
                    )
                    files_input = gr.File(
                        label="ğŸ“ Upload Files",
                        file_count="multiple",
                        file_types=["text", "pdf", "docx", "markdown"]
                    )
                    rag_type = gr.Radio(
                        choices=["RAG", "GraphRAG"],
                        value="GraphRAG",
                        label="ğŸ§  RAG Type",
                        info="GraphRAG builds knowledge graphs, regular RAG uses vector similarity"
                    )
                    build_btn = gr.Button("ğŸš€ Build Knowledge Base", variant="primary")
                
                with gr.Column(scale=1):
                    build_output = gr.Textbox(
                        label="ğŸ“Š Build Status",
                        lines=15,
                        max_lines=20
                    )
        
        with gr.Tab("â“ Ask Questions"):
            with gr.Row():
                with gr.Column(scale=2):
                    # Text input
                    question_input = gr.Textbox(
                        label="â“ Your Question (Text)",
                        placeholder="Ask anything about your uploaded documents...",
                        lines=3
                    )
                    
                    # Voice input
                    gr.Markdown("### ğŸ¤ Voice Input")
                    voice_input = gr.Audio(
                        label="ğŸ™ï¸ Record your question",
                        type="numpy",
                        show_label=True
                    )
                    
                    transcribe_btn = gr.Button("ğŸ“ Transcribe Voice", variant="secondary")
                    transcribed_text = gr.Textbox(
                        label="ğŸ“ Transcribed Text",
                        placeholder="Transcribed text will appear here...",
                        lines=2
                    )
                    
                    # Control buttons
                    with gr.Row():
                        ask_btn = gr.Button("ğŸ” Ask Question (Text)", variant="primary")
                        ask_voice_btn = gr.Button("ğŸ¤ Ask Question (Voice)", variant="primary")
                    
                    # TTS option
                    enable_tts = gr.Checkbox(
                        label="ğŸ”Š Enable Text-to-Speech Output",
                        value=True,
                        info="Convert answers to speech"
                    )
                    
                with gr.Column(scale=1):
                    status_btn = gr.Button("ğŸ“Š Check Status")
                    status_output = gr.Textbox(
                        label="ğŸ”„ System Status",
                        lines=10,
                        max_lines=15
                    )
            
            answer_output = gr.Textbox(
                label="ğŸ’¡ Answer",
                lines=15,
                max_lines=25
            )
        
        with gr.Tab("ğŸ”§ System Info"):
            gr.Markdown("## System Configuration")
            
            info_text = f"""
            **ğŸ¤– Ollama Configuration:**
            - Base URL: `{OLLAMA_BASE_URL}`
            - Model: `{OLLAMA_MODEL}`
            
            **ğŸ—„ï¸ Neo4j Configuration:**
            - URI: `{NEO4J_URI}`
            - User: `{NEO4J_USER}`
            - Password: `{'*' * len(NEO4J_PASSWORD) if NEO4J_PASSWORD else 'Not Set'}`
            
            **ğŸ“ Storage Directories:**
            - Graph DB: `{BASE_PERSIST_DIR}`
            - Vector DB: `{BASE_VECTOR_DIR}`
            
            **ğŸ”§ Device:**
            - Current Device: `{device}`
            - CUDA Available: `{torch.cuda.is_available()}`
            
            **ğŸ“¦ Module Status:**
              - Missing Modules: `{', '.join(missing_modules) if missing_modules else 'All modules available'}`
            """
            
            gr.Markdown(info_text)
            
            # Add system check button
            check_system_btn = gr.Button("ğŸ” Check All Systems", variant="secondary")
            system_check_output = gr.Textbox(
                label="ğŸ” System Check Results",
                lines=10,
                placeholder="Click 'Check All Systems' to see detailed status..."
            )
        
        # Event handlers
        def handle_build_knowledge_base(urls, files, rag_type, progress=gr.Progress()):
            """Wrapper for handle_inputs with progress tracking"""
            for result in handle_inputs(urls, files, rag_type, progress):
                yield result
        
        def handle_transcribe(audio):
            """Handle voice transcription"""
            if audio is None:
                return "âŒ No audio provided"
            
            result = process_voice_input(audio)
            return result
        
        def handle_ask_text(question, enable_tts):
            """Handle text-based questions"""
            if not question.strip():
                return "âŒ Please enter a question"
            
            for response in on_ask_with_voice(question, enable_tts):
                yield response
        
        def handle_ask_voice(transcribed_text, enable_tts):
            """Handle voice-based questions"""
            if not transcribed_text or not transcribed_text.strip():
                return "âŒ Please transcribe your voice input first"
            
            for response in on_ask_with_voice(transcribed_text, enable_tts):
                yield response
        
        def handle_system_check():
            """Perform comprehensive system check"""
            results = []
            
            # Check Ollama
            ollama_ok, ollama_msg = check_ollama_connection()
            results.append(f"ğŸ¤– Ollama: {ollama_msg}")
            
            # Check Neo4j
            if neo4j_driver:
                try:
                    with neo4j_driver.session() as session:
                        result = session.run("RETURN 1 as test")
                        record = result.single()
                        if record and record["test"] == 1:
                            results.append("ğŸ—„ï¸ Neo4j: âœ… Connected and responding")
                        else:
                            results.append("ğŸ—„ï¸ Neo4j: âš ï¸ Connected but unexpected response")
                except Exception as e:
                    results.append(f"ğŸ—„ï¸ Neo4j: âŒ Connection error: {str(e)}")
            else:
                results.append("ğŸ—„ï¸ Neo4j: âŒ Not connected")
            
            # Check voice components
            results.append(f"ğŸ¤ Whisper: {'âœ… Loaded' if whisper_model else 'âŒ Not loaded'}")
            results.append(f"ğŸ”Š Audio System: {'âœ… Ready' if pygame_initialized else 'âŒ Not ready'}")
            
            # Check directories
            results.append(f"ğŸ“ Graph DB Dir: {'âœ… Exists' if os.path.exists(BASE_PERSIST_DIR) else 'âŒ Missing'}")
            results.append(f"ğŸ“ Vector DB Dir: {'âœ… Exists' if os.path.exists(BASE_VECTOR_DIR) else 'âŒ Missing'}")
            
            # Check current RAG status
            if current_rag_type:
                results.append(f"ğŸ§  Current RAG Type: {current_rag_type}")
                if current_rag_type == "GraphRAG" and current_persist_dir:
                    results.append(f"ğŸ“Š GraphRAG Dir: {'âœ… Exists' if os.path.exists(current_persist_dir) else 'âŒ Missing'}")
                elif current_rag_type == "RAG" and current_vector_dir:
                    results.append(f"ğŸ“Š Vector Store Dir: {'âœ… Exists' if os.path.exists(current_vector_dir) else 'âŒ Missing'}")
            else:
                results.append("ğŸ§  RAG System: âš ï¸ No knowledge base built yet")
            
            # Check GPU/CUDA
            if torch.cuda.is_available():
                results.append(f"ğŸš€ CUDA: âœ… Available (Device: {torch.cuda.get_device_name()})")
                results.append(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB")
            else:
                results.append("ğŸš€ CUDA: âŒ Not available (using CPU)")
            
            return "\n".join(results)
        
        # Wire up the event handlers
        build_btn.click(
            fn=handle_build_knowledge_base,
            inputs=[urls_input, files_input, rag_type],
            outputs=[build_output]
        )
        
        transcribe_btn.click(
            fn=handle_transcribe,
            inputs=[voice_input],
            outputs=[transcribed_text]
        )
        
        ask_btn.click(
            fn=handle_ask_text,
            inputs=[question_input, enable_tts],
            outputs=[answer_output]
        )
        
        ask_voice_btn.click(
            fn=handle_ask_voice,
            inputs=[transcribed_text, enable_tts],
            outputs=[answer_output]
        )
        
        status_btn.click(
            fn=get_current_status,
            inputs=[],
            outputs=[status_output]
        )
        
        check_system_btn.click(
            fn=handle_system_check,
            inputs=[],
            outputs=[system_check_output]
        )
        
        # Auto-populate transcribed text to question input
        def copy_transcribed_to_question(transcribed):
            return transcribed
        
        transcribed_text.change(
            fn=copy_transcribed_to_question,
            inputs=[transcribed_text],
            outputs=[question_input]
        )
        
    return interface

def cleanup_resources():
    """Clean up system resources"""
    global neo4j_driver, pygame_initialized, whisper_model
    
    try:
        if neo4j_driver:
            neo4j_driver.close()
            print("âœ… Neo4j connection closed")
    except Exception as e:
        print(f"âš ï¸ Error closing Neo4j: {e}")
    
    try:
        if pygame_initialized:
            pygame.mixer.quit()
            print("âœ… Pygame mixer closed")
    except Exception as e:
        print(f"âš ï¸ Error closing pygame: {e}")
    
    # Clear CUDA cache if available
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("âœ… CUDA cache cleared")
    except Exception as e:
        print(f"âš ï¸ Error clearing CUDA cache: {e}")
    
    # Force garbage collection
    gc.collect()
    print("âœ… Garbage collection completed")

def main():
    """Main function to run the application"""
    print("ğŸš€ Starting RAG & Graph RAG System with Voice...")
    
    try:
        # Initialize voice components
        print("ğŸ¤ Initializing voice components...")
        initialize_voice_components()
        
        # Create directories if they don't exist
        os.makedirs(BASE_PERSIST_DIR, exist_ok=True)
        os.makedirs(BASE_VECTOR_DIR, exist_ok=True)
        print(f"ğŸ“ Created directories: {BASE_PERSIST_DIR}, {BASE_VECTOR_DIR}")
        
        # Check initial system status
        print("ğŸ” Checking system status...")
        ollama_ok, ollama_msg = check_ollama_connection()
        print(f"ğŸ¤– {ollama_msg}")
        
        if neo4j_driver:
            print("ğŸ—„ï¸ Neo4j: âœ… Connected")
        else:
            print("ğŸ—„ï¸ Neo4j: âŒ Not connected (optional for RAG)")
        
        # Create and launch interface
        print("ğŸŒ Creating Gradio interface...")
        interface = create_interface()
        
        print("âœ… System initialized successfully!")
        print("ğŸš€ Launching web interface...")
        print(f"ğŸ“± Access the interface at: http://localhost:7860")
        print("ğŸ”„ Use Ctrl+C to stop the server")
        
        # Launch with specific configuration
        interface.launch(
            server_name="127.0.0.1",  # Allow external access
            server_port=7860,
            share=False,  # Set to True if you want a public link
            debug=False,
            show_error=True,
            quiet=False
        )
        
    except KeyboardInterrupt:
        print("\nğŸ”„ Shutting down gracefully...")
    except Exception as e:
        print(f"âŒ Error during startup: {str(e)}")
        traceback.print_exc()
    finally:
        print("ğŸ§¹ Cleaning up resources...")
        cleanup_resources()
        print("ğŸ‘‹ Goodbye!")

if __name__ == "__main__":
    main()